{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5134accc",
   "metadata": {},
   "source": [
    "# LLM Route Feature Editor (Clean)\n",
    "\n",
    "Minimal notebook focused on preparing interpretable route features, prompting an LLM, and applying its transformation plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2777a685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.llms import HuggingFaceHub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9f2c11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7717 routes\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HFT\")\n",
    "assert hf_token, \"HFT token missing in .env\"\n",
    "\n",
    "df = pd.read_csv(\"/Users/eugeneleach/code/Eugle3/cycle_more/Notebooks/UK_Engineered_Data.csv\")\n",
    "print(f\"Loaded {len(df)} routes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca276862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0.1                                                  6\n",
       "Unnamed: 0                                                   56\n",
       "id                                                      1959833\n",
       "name                                  East Kilkenny Cycle Route\n",
       "distance_m                                                  6.7\n",
       "duration_s                                                  1.3\n",
       "ascent_m                                                    0.0\n",
       "descent_m                                                   0.0\n",
       "steps                                                         2\n",
       "turns                                                         0\n",
       "Asphalt                                                   100.0\n",
       "Unknown                                                     0.0\n",
       "Paved                                                       0.0\n",
       "Compacted Gravel                                            0.0\n",
       "Wood                                                        0.0\n",
       "Gravel                                                      0.0\n",
       "Paving Stones                                               0.0\n",
       "Ground                                                      0.0\n",
       "Concrete                                                    0.0\n",
       "Grass                                                       0.0\n",
       "Metal                                                       0.0\n",
       "Unpaved                                                     0.0\n",
       "Dirt                                                        0.0\n",
       "Grass Paver                                                 0.0\n",
       "Sand                                                        0.0\n",
       "Road                                                      100.0\n",
       "Cycleway                                                    0.0\n",
       "State Road                                                  0.0\n",
       "Track                                                       0.0\n",
       "Street                                                      0.0\n",
       "Path                                                        0.0\n",
       "Footway                                                     0.0\n",
       "Unknown.1                                                   0.0\n",
       "Steps                                                       0.0\n",
       "Construction                                                0.0\n",
       "Ferry                                                       0.0\n",
       "uphill_very_steep (7% to 10%)                               0.0\n",
       "uphill_moderate (3% to 5%)                                  0.0\n",
       "uphill_gentle (0% to 3%)                                    0.0\n",
       "flat (0%)                                                   0.0\n",
       "downhill_gentle (-5% to 0%)                                 0.0\n",
       "uphill_steep (5% to 7%)                                     0.0\n",
       "uphill_extreme (>10%)                                       0.0\n",
       "downhill_extreme (<-15%)                                    0.0\n",
       "downhill_moderate (-7% to -5%)                              0.0\n",
       "downhill_steep (-10% to -7%)                                0.0\n",
       "downhill_very_steep (-15% to -10%)                          0.0\n",
       "Average_Speed                                          5.153846\n",
       "Turn_Density                                                0.0\n",
       "Name: 6, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b804043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_interpretable_features(row):\n",
    "    \"\"\"\n",
    "    Converts a full engineered feature row into an interpretable, LLM-friendly\n",
    "    feature dictionary (Option B: grouped/aggregated features).\n",
    "    \"\"\"\n",
    "\n",
    "    features = {\n",
    "        \"distance_m\": float(row[\"distance_m\"]),\n",
    "        \"duration_s\": float(row[\"duration_s\"]),\n",
    "        \"ascent_m\": float(row[\"ascent_m\"]),\n",
    "        \"descent_m\": float(row[\"descent_m\"])\n",
    "    }\n",
    "\n",
    "    surface_map = {\n",
    "        \"paved_percent\": [\"Asphalt\", \"Paved\", \"Concrete\"],\n",
    "        \"gravel_percent\": [\"Gravel\", \"Compacted Gravel\"],\n",
    "        \"dirt_percent\": [\"Dirt\", \"Unpaved\", \"Ground\"],\n",
    "        \"grass_percent\": [\"Grass\", \"Grass Paver\"],\n",
    "        \"other_percent\": [\"Wood\", \"Metal\", \"Sand\", \"Paving Stones\", \"Unknown\"],\n",
    "    }\n",
    "    # The total percentage of the route that is paved (asphalt, paved, concrete) for example\n",
    "    surface_profile = {k: float(sum(row[col] for col in cols)) for k, cols in surface_map.items()}\n",
    "\n",
    "    hill_map = {\n",
    "        \"flat_percent\": [\"flat (0%)\"],\n",
    "        \"gentle_uphill_percent\": [\"uphill_gentle (0% to 3%)\"],\n",
    "        \"moderate_uphill_percent\": [\"uphill_moderate (3% to 5%)\"],\n",
    "        \"steep_uphill_percent\": [\"uphill_steep (5% to 7%)\", \"uphill_very_steep (7% to 10%)\", \"uphill_extreme (>10%)\"],\n",
    "        \"gentle_downhill_percent\": [\"downhill_gentle (-5% to 0%)\"],\n",
    "        \"steep_downhill_percent\": [\"downhill_moderate (-7% to -5%)\", \"downhill_steep (-10% to -7%)\", \"downhill_very_steep (-15% to -10%)\", \"downhill_extreme (<-15%)\"],\n",
    "    }\n",
    "    hill_profile = {k: float(sum(row[col] for col in cols)) for k, cols in hill_map.items()}\n",
    "\n",
    "    route_shape = {\n",
    "        \"turns\": float(row[\"turns\"]),\n",
    "        \"steps\": float(row[\"steps\"]),\n",
    "        \"turn_density\": float(row[\"Turn_Density\"]),\n",
    "        \"avg_speed\": float(row[\"Average_Speed\"]),\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"distance_m\": features[\"distance_m\"],\n",
    "        \"duration_s\": features[\"duration_s\"],\n",
    "        \"ascent_m\": features[\"ascent_m\"],\n",
    "        \"descent_m\": features[\"descent_m\"],\n",
    "        \"surface_profile\": surface_profile,\n",
    "        \"hill_profile\": hill_profile,\n",
    "        \"route_shape\": route_shape,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a01b8aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "route_edit_prompt = PromptTemplate(\n",
    "    input_variables=[\"features\", \"instruction\"],\n",
    "    template=\"\"\"\n",
    "You are an assistant that modifies cycling route features based on a user's instruction.\n",
    "\n",
    "You will receive:\n",
    "1. An interpretable cycling route feature dictionary.\n",
    "2. A natural-language instruction from the user.\n",
    "\n",
    "Your job:\n",
    "- Understand the user's intent.\n",
    "- Create a JSON object describing how each feature should be changed.\n",
    "- Only modify features that are relevant to the user's request.\n",
    "- Do NOT produce any explanation or commentary — output JSON ONLY.\n",
    "\n",
    "The JSON format must be:\n",
    "\n",
    "{{\n",
    "  \"distance_m\": {{\"operation\": \"<set/add/multiply/none>\", \"value\": <number>}},\n",
    "  \"duration_s\": {{\"operation\": \"<set/add/multiply/none>\", \"value\": <number>}},\n",
    "  \"ascent_m\": {{\"operation\": \"<set/add/multiply/none>\", \"value\": <number>}},\n",
    "  \"descent_m\": {{\"operation\": \"<set/add/multiply/none>\", \"value\": <number>}},\n",
    "  \"surface_profile\": {{\n",
    "    \"paved_percent\": {{\"operation\": \"<set/add/multiply/none>\", \"value\": <number>}},\n",
    "    \"gravel_percent\": {{\"operation\": \"<set/add/multiply/none>\", \"value\": <number>}},\n",
    "    \"dirt_percent\": {{\"operation\": \"<set/add/multiply/none>\", \"value\": <number>}},\n",
    "    \"grass_percent\": {{\"operation\": \"<set/add/multiply/none>\", \"value\": <number>}},\n",
    "    \"other_percent\": {{\"operation\": \"<set/add/multiply/none>\", \"value\": <number>}}\n",
    "  }},\n",
    "  \"hill_profile\": {{\n",
    "    \"flat_percent\": {{\"operation\": \"<set/add/multiply/none>\", \"value\": <number>}},\n",
    "    \"gentle_uphill_percent\": {{\"operation\": \"<set/add/multiply/none>\", \"value\": <number>}},\n",
    "    \"moderate_uphill_percent\": {{\"operation\": \"<set/add/multiply/none>\", \"value\": <number>}},\n",
    "    \"steep_uphill_percent\": {{\"operation\": \"<set/add/multiply/none>\", \"value\": <number>}},\n",
    "    \"gentle_downhill_percent\": {{\"operation\": \"<set/add/multiply/none>\", \"value\": <number>}},\n",
    "    \"steep_downhill_percent\": {{\"operation\": \"<set/add/multiply/none>\", \"value\": <number>}}\n",
    "  }},\n",
    "  \"route_shape\": {{\n",
    "    \"turns\": {{\"operation\": \"<set/add/multiply/none>\", \"value\": <number>}},\n",
    "    \"steps\": {{\"operation\": \"<set/add/multiply/none>\", \"value\": <number>}},\n",
    "    \"turn_density\": {{\"operation\": \"<set/add/multiply/none>\", \"value\": <number>}},\n",
    "    \"avg_speed\": {{\"operation\": \"<set/add/multiply/none>\", \"value\": <number>}}\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Rules:\n",
    "- Use \"set\" to directly assign a value.\n",
    "- Use \"add\" or \"multiply\" for relative changes.\n",
    "- Use \"none\" to leave a field unchanged.\n",
    "- Keep values within realistic bounds (e.g., paved_percent between 0 and 100).\n",
    "- Ensure surface_profile and hill_profile percentages sum to ~100 each.\n",
    "- Keep turns and steps non-negative.\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "202d5701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#op dict {\"operation\": \"<set/add/multiply/none>\", \"value\": <number>}\n",
    "def apply_operation(original_value, op_dict):\n",
    "    operation = op_dict.get(\"operation\", \"none\")\n",
    "    value = op_dict.get(\"value\", 0)\n",
    "    if operation == \"none\":\n",
    "        return original_value\n",
    "    if operation == \"set\":\n",
    "        return float(value)\n",
    "    if operation == \"add\":\n",
    "        return float(original_value) + float(value)\n",
    "    if operation == \"multiply\":\n",
    "        return float(original_value) * float(value)\n",
    "    return original_value\n",
    "\n",
    "\n",
    "def apply_transformation_plan(original_features, plan):\n",
    "    updated = {}\n",
    "    for key in [\"distance_m\", \"duration_s\", \"ascent_m\", \"descent_m\"]:\n",
    "        updated[key] = apply_operation(original_features[key], plan[key])\n",
    "\n",
    "    updated_surface = {\n",
    "        k: apply_operation(original_features[\"surface_profile\"][k], plan[\"surface_profile\"][k])\n",
    "        for k in original_features[\"surface_profile\"].keys()\n",
    "    }\n",
    "    total = sum(updated_surface.values())\n",
    "    if total > 0:\n",
    "        updated_surface = {k: (v / total) * 100 for k, v in updated_surface.items()}\n",
    "\n",
    "    updated_hill = {\n",
    "        k: apply_operation(original_features[\"hill_profile\"][k], plan[\"hill_profile\"][k])\n",
    "        for k in original_features[\"hill_profile\"].keys()\n",
    "    }\n",
    "    total_h = sum(updated_hill.values())\n",
    "    if total_h > 0:\n",
    "        updated_hill = {k: (v / total_h) * 100 for k, v in updated_hill.items()}\n",
    "\n",
    "    updated_shape = {\n",
    "        k: apply_operation(original_features[\"route_shape\"][k], plan[\"route_shape\"][k])\n",
    "        for k in original_features[\"route_shape\"].keys()\n",
    "    }\n",
    "    updated_shape[\"turns\"] = max(0, updated_shape[\"turns\"])\n",
    "    updated_shape[\"steps\"] = max(0, updated_shape[\"steps\"])\n",
    "\n",
    "    updated[\"surface_profile\"] = updated_surface\n",
    "    updated[\"hill_profile\"] = updated_hill\n",
    "    updated[\"route_shape\"] = updated_shape\n",
    "    return updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "839ccd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b4ff630",
   "metadata": {},
   "outputs": [],
   "source": [
    "GKEY = os.getenv(\"GEMINIKEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef7a0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM ready (patched InferenceClient.post)\n"
     ]
    }
   ],
   "source": [
    "# Build LLM client for conversational task\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"TeichAI/Qwen3-4B-Thinking-2507-Gemini-2.5-Flash-Distill-GGUF\",\n",
    "    task=\"conversational\",\n",
    "    huggingfacehub_api_token=hf_token,\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_new_tokens\": 512,\n",
    "    },\n",
    ")\n",
    "\n",
    "# # Patch InferenceClient to match old .post interface expected by HuggingFaceHub\n",
    "# import json as _json\n",
    "\n",
    "# def _post_patch(json=None, task=None):\n",
    "#     prompt = json[\"inputs\"]\n",
    "#     params = json.get(\"parameters\", {})\n",
    "#     res = llm.client.text_generation(prompt, **params)\n",
    "#     text = res if isinstance(res, str) else res.generated_text\n",
    "#     return _json.dumps([{ \"generated_text\": text }]).encode()\n",
    "\n",
    "# llm.client.post = _post_patch\n",
    "# print(\"✅ LLM ready (patched InferenceClient.post)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30d9297b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou are an assistant that modifies cycling route features based on a user\\'s instruction.\\n\\nYou will receive:\\n1. An interpretable cycling route feature dictionary.\\n2. A natural-language instruction from the user.\\n\\nYour job:\\n- Understand the user\\'s intent.\\n- Create a JSON object describing how each feature should be changed.\\n- Only modify features that are relevant to the user\\'s request.\\n- Do NOT produce any explanation or commentary — output JSON ONLY.\\n\\nThe JSON format must be:\\n\\n{\\n  \"distance_m\": {\"operation\": \"<set/add/multiply/none>\", \"value\": <number>},\\n  \"duration_s\": {\"operation\": \"<set/add/multiply/none>\", \"value\": <number>},\\n  \"ascent_m\": {\"operation\": \"<set/add/multiply/none>\", \"value\": <number>},\\n  \"descent_m\": {\"operation\": \"<set/add/multiply/none>\", \"value\": <number>},\\n  \"surface_profile\": {\\n    \"paved_percent\": {\"operation\": \"<set/add/multiply/none>\", \"value\": <number>},\\n    \"gravel_percent\": {\"operation\": \"<set/add/multiply/none>\", \"value\": <number>},\\n    \"dirt_percent\": {\"operation\": \"<set/add/multiply/none>\", \"value\": <number>},\\n    \"grass_percent\": {\"operation\": \"<set/add/multiply/none>\", \"value\": <number>},\\n    \"other_percent\": {\"operation\": \"<set/add/multiply/none>\", \"value\": <number>}\\n  },\\n  \"hill_profile\": {\\n    \"flat_percent\": {\"operation\": \"<set/add/multiply/none>\", \"value\": <number>},\\n    \"gentle_uphill_percent\": {\"operation\": \"<set/add/multiply/none>\", \"value\": <number>},\\n    \"moderate_uphill_percent\": {\"operation\": \"<set/add/multiply/none>\", \"value\": <number>},\\n    \"steep_uphill_percent\": {\"operation\": \"<set/add/multiply/none>\", \"value\": <number>},\\n    \"gentle_downhill_percent\": {\"operation\": \"<set/add/multiply/none>\", \"value\": <number>},\\n    \"steep_downhill_percent\": {\"operation\": \"<set/add/multiply/none>\", \"value\": <number>}\\n  },\\n  \"route_shape\": {\\n    \"turns\": {\"operation\": \"<set/add/multiply/none>\", \"value\": <number>},\\n    \"steps\": {\"operation\": \"<set/add/multiply/none>\", \"value\": <number>},\\n    \"turn_density\": {\"operation\": \"<set/add/multiply/none>\", \"value\": <number>},\\n    \"avg_speed\": {\"operation\": \"<set/add/multiply/none>\", \"value\": <number>}\\n  }\\n}\\n\\nRules:\\n- Use \"set\" to directly assign a value.\\n- Use \"add\" or \"multiply\" for relative changes.\\n- Use \"none\" to leave a field unchanged.\\n- Keep values within realistic bounds (e.g., paved_percent between 0 and 100).\\n- Ensure surface_profile and hill_profile percentages sum to ~100 each.\\n- Keep turns and steps non-negative.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample a route, build prompt, and apply plan\n",
    "row = df.sample(1).iloc[0]\n",
    "features = extract_interpretable_features(row)\n",
    "\n",
    "instruction = \"Make it about 2x longer, mostly paved, fewer turns\"\n",
    "\n",
    "prompt = route_edit_prompt.format(\n",
    "    features=json.dumps(features, indent=2),\n",
    "    instruction=instruction,\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "203beab3",
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mStopIteration\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m raw_plan = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/cyclemore/lib/python3.12/site-packages/langchain_core/language_models/llms.py:373\u001b[39m, in \u001b[36mBaseLLM.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    364\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    369\u001b[39m     **kwargs: Any,\n\u001b[32m    370\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    371\u001b[39m     config = ensure_config(config)\n\u001b[32m    372\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    383\u001b[39m         .generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m    384\u001b[39m         .text\n\u001b[32m    385\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/cyclemore/lib/python3.12/site-packages/langchain_core/language_models/llms.py:784\u001b[39m, in \u001b[36mBaseLLM.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    775\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    777\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    781\u001b[39m     **kwargs: Any,\n\u001b[32m    782\u001b[39m ) -> LLMResult:\n\u001b[32m    783\u001b[39m     prompt_strings = [p.to_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/cyclemore/lib/python3.12/site-packages/langchain_core/language_models/llms.py:1006\u001b[39m, in \u001b[36mBaseLLM.generate\u001b[39m\u001b[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    988\u001b[39m     run_managers = [\n\u001b[32m    989\u001b[39m         callback_manager.on_llm_start(\n\u001b[32m    990\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1004\u001b[39m         )\n\u001b[32m   1005\u001b[39m     ]\n\u001b[32m-> \u001b[39m\u001b[32m1006\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) > \u001b[32m0\u001b[39m:\n\u001b[32m   1014\u001b[39m     run_managers = [\n\u001b[32m   1015\u001b[39m         callback_managers[idx].on_llm_start(\n\u001b[32m   1016\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1023\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[32m   1024\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/cyclemore/lib/python3.12/site-packages/langchain_core/language_models/llms.py:810\u001b[39m, in \u001b[36mBaseLLM._generate_helper\u001b[39m\u001b[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[39m\n\u001b[32m    799\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_helper\u001b[39m(\n\u001b[32m    800\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    801\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    806\u001b[39m     **kwargs: Any,\n\u001b[32m    807\u001b[39m ) -> LLMResult:\n\u001b[32m    808\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    809\u001b[39m         output = (\n\u001b[32m--> \u001b[39m\u001b[32m810\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    811\u001b[39m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    813\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[32m    814\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    815\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    816\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    817\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    818\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate(prompts, stop=stop)\n\u001b[32m    819\u001b[39m         )\n\u001b[32m    820\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    821\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/cyclemore/lib/python3.12/site-packages/langchain_core/language_models/llms.py:1500\u001b[39m, in \u001b[36mLLM._generate\u001b[39m\u001b[34m(self, prompts, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1497\u001b[39m new_arg_supported = inspect.signature(\u001b[38;5;28mself\u001b[39m._call).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1498\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[32m   1499\u001b[39m     text = (\n\u001b[32m-> \u001b[39m\u001b[32m1500\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1501\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m   1502\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(prompt, stop=stop, **kwargs)\n\u001b[32m   1503\u001b[39m     )\n\u001b[32m   1504\u001b[39m     generations.append([Generation(text=text)])\n\u001b[32m   1505\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations=generations)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/cyclemore/lib/python3.12/site-packages/langchain_community/llms/huggingface_hub.py:138\u001b[39m, in \u001b[36mHuggingFaceHub._call\u001b[39m\u001b[34m(self, prompt, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    135\u001b[39m _model_kwargs = \u001b[38;5;28mself\u001b[39m.model_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m    136\u001b[39m parameters = {**_model_kwargs, **kwargs}\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minputs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparameters\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtask\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m response = json.loads(response.decode())\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36m_post_patch\u001b[39m\u001b[34m(json, task)\u001b[39m\n\u001b[32m     16\u001b[39m prompt = json[\u001b[33m\"\u001b[39m\u001b[33minputs\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     17\u001b[39m params = json.get(\u001b[33m\"\u001b[39m\u001b[33mparameters\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m res = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m text = res \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m res.generated_text\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _json.dumps([{ \u001b[33m\"\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m\"\u001b[39m: text }]).encode()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/cyclemore/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:2373\u001b[39m, in \u001b[36mInferenceClient.text_generation\u001b[39m\u001b[34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[39m\n\u001b[32m   2367\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2368\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mAPI endpoint/model for text-generation is not served via TGI. Cannot return output as a stream.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2369\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m Please pass `stream=False` as input.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2370\u001b[39m         )\n\u001b[32m   2372\u001b[39m model_id = model \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model\n\u001b[32m-> \u001b[39m\u001b[32m2373\u001b[39m provider_helper = \u001b[43mget_provider_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-generation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2374\u001b[39m request_parameters = provider_helper.prepare_request(\n\u001b[32m   2375\u001b[39m     inputs=prompt,\n\u001b[32m   2376\u001b[39m     parameters=parameters,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2380\u001b[39m     api_key=\u001b[38;5;28mself\u001b[39m.token,\n\u001b[32m   2381\u001b[39m )\n\u001b[32m   2383\u001b[39m \u001b[38;5;66;03m# Handle errors separately for more precise error messages\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/cyclemore/lib/python3.12/site-packages/huggingface_hub/inference/_providers/__init__.py:245\u001b[39m, in \u001b[36mget_provider_helper\u001b[39m\u001b[34m(provider, task, model)\u001b[39m\n\u001b[32m    242\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m CONVERSATIONAL_AUTO_ROUTER\n\u001b[32m    244\u001b[39m     provider_mapping = _fetch_inference_provider_mapping(model)\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m     provider = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprovider_mapping\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.provider\n\u001b[32m    247\u001b[39m provider_tasks = PROVIDERS.get(provider)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m provider_tasks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mStopIteration\u001b[39m: "
     ]
    }
   ],
   "source": [
    "raw_plan = llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416db2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_plan = llm.invoke(prompt)\n",
    "plan = json.loads(raw_plan)\n",
    "updated = apply_transformation_plan(features, plan)\n",
    "print(\"Original features:\", json.dumps(features, indent=2))\n",
    "print(\"Plan:\", json.dumps(plan, indent=2))\n",
    "print(\"Updated features:\", json.dumps(updated, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5835b1f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyclemore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
